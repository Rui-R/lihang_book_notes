# ch5 决策树

决策树(decision tree)是一种基本的**分类**与**回归**方法，呈树形结构。在分类问题中，表示基于特征对实例进行分类的过程，它可以认为是==if-then规则的集合==，也可以认为是==定义在特征空间与类空间上的条件概率分布==。



## 5.1 决策树模型与学习

决策树学习通常包括3个步骤：**特征选择**、**决策树的生成**和**决策树的修剪**。

模型的主要优点：具有可读性，分类速度快。

### 5.1.1 决策树模型

> 定义：分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点分为内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。
>

### 5.1.2 if-then规则

- 决策树可以看成一个if-then规则的集合：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上**内部结点的特征**对应着规则的**条件**，**叶结点的类**对应着规则的**结论**。
- 路径总是**互斥且完备**的。

### 5.1.3 决策树与条件概率分布

- 决策树还表示给定特征条件下类的条件概率分布，这一条件概率分布定义在特征空间的一个划分上。
- 特征空间划分为互不相交的单元，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。
- 决策树分类时将结点的实例分到**条件概率大**的那一类去。

### 5.1.4 决策树学习

- 决策树学习本质上是从训练数据集中归纳出一组分类规则，找出一个与训练数据矛盾较小，同时具有较好泛化能力的决策树。
- 决策树学习的损失函数通常是正则化的极大似然函数，策略是以损失函数为目标函数的最小化。
- 从所有可能决策树中选取最优决策树是NP完全问题，因此常用启发式方法近似求解出次优决策树。
- 决策树的学习算法主要有ID3，C4.5和CART。
- 决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优。



## 5.2 特征选择

特征选择在于选取对训练数据具有分类能力的特征，以提高决策树学习的效率。特征选择的准则有**信息增益**，**信息增益比**或**基尼指数**。

## 概念



#### 熵和条件熵

##### 熵

熵是表示随机变量不确定性的度量。熵越大，随机变量的不确定性越大。
$$
H(p)=H(X)=-\sum_{i=1}^{n}p_i\log p_i
$$

- 对数以2为底时，熵的单位是比特；以e为底时，熵的单位是纳特。
- 熵只与$X$的**分布**有关，与$X$取值无关。
- 定义$0\log0=0$，熵是非负的。

##### 条件熵

条件熵$H(Y|X)$定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望。

随机变量$(X,Y)$的联合概率分布为

$P(X=x_i,Y=y_j)=p_{ij}, i=1,2,\dots ,n;j=1,2,\dots ,m$

条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性
$$
H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)
$$
其中$p_i=P(X=x_i),i=1,2,\dots ,n$



#### 经验熵和经验条件熵

> 当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵。

就是从**已知的数据**计算得到的结果。



#### 信息增益和信息增益比	

##### 信息增益

信息增益表示得知特征X的信息而使得类Y的信息的不确定减少的程度。

特征$A$对训练数据集$D$的信息增益$g(D|A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定的条件下$D$的经验条件熵$H(D|A)$之差：
$$
g(D,A)=H(D)-H(D|A)
$$
熵与条件熵的差称为互信息.

决策树中的信息增益等价于训练数据集中的类与特征的互信息。

考虑ID这种特征， 本身是唯一的。按照ID做划分， 得到的经验条件熵为0, 会得到最大的信息增益。所以， 按照信息增益的准则来选择特征， 可能会倾向于**取值比较多**的特征。

##### 信息增益比

特征A对训练集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\
H_A(D)=-\sum_{i=1}^n\frac{D_i}{D}log_2\frac{D_i}{D}
$$


#### 基尼指数

分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为：
$$
Gini(p) = \sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$

- 在二类分类问题中，若样本点属于第1类的概率为$p$，则概率分布的基尼指数为：$Gini(p) = 2p(1-p)$。
- 基尼指数越大，样本集合的不确定性就越大，这一点与熵类似。



## 5.3 生成决策树

### 信息增益

> 输入：训练数据集$D$和特征$A$
>
> 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$
>
> 1. 数据集$D$的经验熵$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$
> 2. 特征$A$对数据集$D$的经验条件熵$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$
> 3. 信息增益$g(D,A)=H(D)-H(D|A)$



### ID3

> 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
> 输出：决策树$T$
>
> 1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$
> 2. 如果$A = \emptyset$，置$T$为单节点树，实例数最多的类$C_k$作为该节点类标记，返回$T$
> 3. 计算$g$, 选择**信息增益**最大的特征$A_g$
> 4. 如果$A_g$的**信息增益**小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
> 5. $A_g$划分若干非空子集$D_i$，
> 6. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$



### C4.5

> 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
> 输出：决策树$T$
>
> 1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$
> 2. 如果$A = \emptyset$, 置$T$为单节点树，实例数最多的类$C_k$作为该节点类标记，返回$T$
> 3. 计算$g$, 选择**信息增益比**最大的特征$A_g$
> 4. 如果$A_g$的**信息增益比**小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
> 5. $A_g$划分若干非空子集$D_i$，
> 6. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$

ID3和C4.5在生成上一致，差异只在准则的差异。



### CART

> 分类与回归树（classification and regression tree CART）是1984年由Leo Breiman提出的，他还留下了Bagging，Random Forest等模型，对统计学界影响很大。值得一提的是，CART是在ID3和C4.5之前提出的，后两者只能处理分类问题，而CART即可以处理分类问题，也可以处理回归问题。

- `sklearn.tree.DecisionTreeClassifier()`中用CART实现了决策树，但没有实现**后剪枝**部分。
- CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。它假设决策树是二叉树，内部结点特征的取值为“是”和“否”，分为**决策树生成**和**决策树剪枝**两部分。



#### 回归树的生成

回归树的生成采用的是**平方误差最小化**准则。

> #### 算法5.5 最小二乘回归树生成
>
> 输入：训练数据集$D$
>
> 输出：回归树$f(x)$
>
> 步骤：
>
> 1. 遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使下式达到最小值的$(j,s)$
>    $$
>    \min\limits_{j,s}\left[\min\limits_{c_1}\sum\limits_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min\limits_{c_2}\sum\limits_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]
>    $$
>
> 2. 用选定的$(j,s)$, 划分区域并决定相应的输出值
>    $$
>    R_1(j,s)=\{x|x^{(j)}\leq s\}, R_2(j,s)=\{x|x^{(j)}> s\} \\
>    \hat{c}_m= \frac{1}{N}\sum\limits_{x_i\in R_m(j,s)} y_j, x\in R_m, m=1,2
>    $$
>
> 3. 对两个子区域调用(1)(2)步骤， 直至满足停止条件
>
> 4. 将输入空间划分为$M$个区域$R_1, R_2,\dots,R_M$，生成决策树：
>    $$
>    f(x)=\sum_{m=1}^M\hat{c}_mI(x\in R_m)
>    $$



#### 分类树的生成

分类树的生成采用的是**基尼指数最小化**准则。

如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即

$D_1 = \{(x,y)\in D|A(x) = a\}$，$D_2 = D - D_1$

则在特征$A$的条件下，集合$D$的基尼指数定义为：

$Gini(D, A) = \frac{|D_1|}{D}Gini(D_1) + \frac{|D_2|}{D}Gini(D_2)$

基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A = a$分割后集合D的不确定性。基尼指数越大，样本集合的不确定性就越大，这一点与熵类似。

## 5.4  剪枝

#### 简单的剪枝

树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$H_t(T)$为叶结点$t$上的经验熵， $\alpha\geqslant 0$为参数，决策树学习的损失函数可以定义为
$$
C_\alpha(T)=\sum_{i=1}^{|T|}N_tH_t(T)+\alpha|T|
$$
其中
$$
H_t(T)=-\sum_k\color{red}\frac{N_{tk}}{N_t}\color{black}\log \frac{N_{tk}}{N_t}
$$

$$
C(T)=\sum_{t=1}^{|T|}\color{red}N_tH_t(T)\color{black}=-\sum_{t=1}^{|T|}\sum_{k=1}^K\color{red}N_{tk}\color{black}\log\frac{N_{tk}}{N_t}
$$

这时有
$$
C_\alpha(T)=C(T)+\alpha|T|
$$
其中$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$\alpha \geqslant 0$控制两者之间的影响。

剪枝，就是在$\alpha$确定的情况下，选择损失函数最小的模型，即损失函数最小的子树。

> 算法5.4 树的剪枝算法
>
> 输入：生成算法生成的整个树$T$，参数$\alpha$
>
> 输出：修剪后的子树$T_\alpha$
>
> 1. 计算每个结点的经验熵
> 2. 递归的从树的叶结点向上回缩
>    假设一组叶结点回缩到其父结点之前与之后的整体树分别是$T_B$和$T_A$，其对应的损失函数分别是$C_\alpha(T_A)$和$C_\alpha(T_B)$，如果$C_\alpha(T_A)\leqslant C_\alpha(T_B)$则进行剪枝，即将父结点变为新的叶结点
> 3. 返回2，直至不能继续为止，得到损失函数最小的子树$T_\alpha$

#### CART剪枝

原理见书P85-P87

> 算法5.7 CART的剪枝算法
>
> 输入：CART算法生成的决策树$T_0$
>
> 输出：最优决策树$T_\alpha$
>
> 1. 设$k = 0，T = T_0$。
>
> 2. 设$\alpha = + \infty$。
>
> 3. 自上而下地对各内部结点$t$计算$C(T_t),|T_t|$以及$g(t) = \frac{C(t) - C(T_t)}{|T_t - 1|}$，$\alpha = min(\alpha,g(t))$。
>
>    这里，$T_t$表示以$t$为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$|T_t|$的叶结点个数。
>
> 4. 对$g(t) = \alpha$的内部结点$t$进行剪枝，并对叶结点$t$以多数表决法决定其类，得到树$T$。
>
> 5. 设$k = k + 1,\alpha_k = \alpha,T_k = T$。
>
> 6. 如果$T_k$不是由根节点及两个叶结点构成的树，则回到步骤2;否则令$T_k = T_n$。
>
> 7. 采用交叉验证法在子树序列$T_0,T_1,···,T_n$中选取最优子树$T_{\alpha}$。

## 5.5 延伸



### 随机森林

随机森林(random forest)